{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Pack: GPT-2 Medium (355M)\n",
    "\n",
    "This notebook runs baseline experiments on **GPT-2 Medium** to test direction specificity.\n",
    "\n",
    "**Model:** GPT-2 Medium (355M parameters, 24 layers)\n",
    "\n",
    "**Baselines tested:**\n",
    "1. **Random directions** (n=10) - Tests if any direction works\n",
    "2. **Shuffled contrast** - Tests if per-prompt pairing matters\n",
    "3. **Benign contrast** - Tests domain specificity\n",
    "\n",
    "**Expected runtime:** ~40-60 minutes on T4 GPU, ~25-35 minutes on A100\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/isahan78/steering-reliability.git\n",
    "%cd steering-reliability\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall any conflicting packages\n",
    "!pip uninstall -y numpy pandas datasets transformer-lens transformers pyarrow scikit-learn -q\n",
    "\n",
    "# Install all dependencies in one command (ensures compatibility)\n",
    "!pip install --no-cache-dir numpy pandas torch transformer-lens transformers datasets matplotlib seaborn pyyaml tqdm pyarrow scikit-learn\n",
    "\n",
    "# Add src to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '/content/steering-reliability/src')\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Verify Imports (CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/steering-reliability/src')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"IMPORT VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"✓ numpy {np.__version__}\")\n",
    "    \n",
    "    from transformer_lens import HookedTransformer\n",
    "    print(f\"✓ transformer_lens.HookedTransformer\")\n",
    "    \n",
    "    from steering_reliability.config import load_config\n",
    "    print(\"✓ config module\")\n",
    "    \n",
    "    from steering_reliability.model import load_model\n",
    "    print(\"✓ model module\")\n",
    "    \n",
    "    from steering_reliability.directions.random_direction import sample_random_direction\n",
    "    print(\"✓ random_direction module\")\n",
    "    \n",
    "    from steering_reliability.directions.build_direction import build_shuffled_contrast_direction\n",
    "    print(\"✓ shuffled_contrast module\")\n",
    "    \n",
    "    from steering_reliability.directions.benign_contrast import build_benign_contrast_direction\n",
    "    print(\"✓ benign_contrast module\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ SUCCESS! All imports work.\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nReady to run baseline pack!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"❌ FAILED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run Baseline Pack - GPT-2 Medium\n",
    "\n",
    "This will:\n",
    "1. Load **GPT-2 Medium** (355M params, 24 layers)\n",
    "2. Build learned direction (from harm_train)\n",
    "3. Build shuffled contrast direction\n",
    "4. Generate 10 random directions\n",
    "5. Build benign contrast direction\n",
    "6. Test all directions with ablation\n",
    "\n",
    "**Settings:**\n",
    "- Model: gpt2-medium\n",
    "- Layer: 16 (middle layer)\n",
    "- Alphas: {0, 1, 4, 8}\n",
    "- Prompts: 50 harm_test, 50 benign\n",
    "- Random trials: 10\n",
    "- Batch size: 4 (reduced for larger model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set PYTHONPATH and run baseline pack experiment\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = '/content/steering-reliability/src'\n",
    "\n",
    "!PYTHONPATH=/content/steering-reliability/src python scripts/run_baseline_pack.py \\\n",
    "  --config configs/gpt2_medium_baseline_pack.yaml \\\n",
    "  --layer 16 \\\n",
    "  --alphas 0 1 4 8 \\\n",
    "  --n_harm_test 50 \\\n",
    "  --n_benign 50 \\\n",
    "  --n_random 10 \\\n",
    "  --seed 0 \\\n",
    "  --include_benign_contrast \\\n",
    "  --output_dir artifacts/baselines_medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate Figures\n",
    "\n",
    "Create visualizations from the baseline pack results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m steering_reliability.analysis.plot_baseline_pack \\\n",
    "  --in_parquet artifacts/baselines_medium/baseline_pack_results.parquet \\\n",
    "  --out_dir artifacts/baselines_medium/figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analyze Results\n",
    "\n",
    "### View Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read summary table\n",
    "table = pd.read_csv('artifacts/baselines_medium/figures/baseline_pack_table.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE PACK SUMMARY TABLE - GPT-2 MEDIUM\")\n",
    "print(\"=\"*80)\n",
    "print(table.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Highlight key findings\n",
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get learned and random at alpha=8\n",
    "learned_8 = table[(table['direction_type'] == 'learned')]\n",
    "random_8 = table[(table['direction_type'] == 'random')]\n",
    "\n",
    "if len(learned_8) > 0:\n",
    "    learned_refusal = learned_8['harm_refusal_α8.0'].values[0]\n",
    "    print(f\"\\n✓ Learned direction (α=8): {learned_refusal} refusal on harm_test\")\n",
    "\n",
    "if len(random_8) > 0:\n",
    "    random_refusal = random_8['harm_refusal_α8.0'].values[0]\n",
    "    print(f\"✓ Random baseline (α=8): {random_refusal} refusal on harm_test\")\n",
    "    \n",
    "    # Parse the mean from \"XX±YY\" format\n",
    "    if isinstance(learned_refusal, str):\n",
    "        learned_val = float(learned_refusal.split('±')[0])\n",
    "    else:\n",
    "        learned_val = float(learned_refusal)\n",
    "    \n",
    "    if isinstance(random_refusal, str):\n",
    "        random_val = float(random_refusal.split('±')[0])\n",
    "    else:\n",
    "        random_val = float(random_refusal)\n",
    "    \n",
    "    gap = learned_val - random_val\n",
    "    print(f\"\\n→ Gap: {gap:+.2f}\")\n",
    "    \n",
    "    if gap > 0.30:\n",
    "        print(\"✅ STRONG: Learned direction is highly specific!\")\n",
    "    elif gap > 0.10:\n",
    "        print(\"⚠️  MODERATE: Direction shows some specificity\")\n",
    "    else:\n",
    "        print(\"❌ WARNING: Random matches learned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Direction Specificity Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIRECTION SPECIFICITY: Refusal on Harm Test\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nLearned (green) should be >> Random (gray) and Shuffled (blue)\\n\")\n",
    "\n",
    "display(Image(filename='artifacts/baselines_medium/figures/direction_specificity.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Benign Preservation Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BENIGN PRESERVATION: Helpfulness on Benign Queries\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAll directions should maintain high helpfulness (>95%)\\n\")\n",
    "\n",
    "display(Image(filename='artifacts/baselines_medium/figures/benign_preservation.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Detailed Analysis\n",
    "\n",
    "Load full results and compute statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load full results\n",
    "df = pd.read_parquet('artifacts/baselines_medium/baseline_pack_results.parquet')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED ANALYSIS (α=8 on harm_test) - GPT-2 MEDIUM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter to alpha=8, harm_test\n",
    "harm_8 = df[(df['alpha'] == 8) & (df['split'] == 'harm_test')]\n",
    "\n",
    "# Compute statistics by direction type\n",
    "stats = harm_8.groupby('direction_type').agg({\n",
    "    'is_refusal': ['mean', 'std', 'count']\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nRefusal rates by direction type:\")\n",
    "print(stats)\n",
    "\n",
    "# For random, show distribution across trials\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANDOM DIRECTION TRIALS (α=8, harm_test)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "random_trials = harm_8[harm_8['direction_type'] == 'random'].groupby('random_trial').agg({\n",
    "    'is_refusal': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(random_trials)\n",
    "print(f\"\\nRandom trials mean: {random_trials['is_refusal'].mean():.3f}\")\n",
    "print(f\"Random trials std: {random_trials['is_refusal'].std():.3f}\")\n",
    "\n",
    "# Compare learned vs random\n",
    "learned_refusal = harm_8[harm_8['direction_type'] == 'learned']['is_refusal'].mean()\n",
    "random_refusal = harm_8[harm_8['direction_type'] == 'random']['is_refusal'].mean()\n",
    "gap = learned_refusal - random_refusal\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LEARNED vs RANDOM GAP\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Learned: {learned_refusal:.1%}\")\n",
    "print(f\"Random:  {random_refusal:.1%}\")\n",
    "print(f\"Gap:     {gap:+.1%}\")\n",
    "\n",
    "if gap > 0.3:\n",
    "    print(\"\\n✅ PASS: Large gap indicates direction is specific!\")\n",
    "elif gap > 0.1:\n",
    "    print(\"\\n⚠️  MODERATE: Gap exists but could be larger\")\n",
    "else:\n",
    "    print(\"\\n❌ FAIL: Random matches learned - effect may not be direction-specific!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Completions Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sample completions to verify quality\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE COMPLETIONS (α=8, harm_test)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for direction_type in ['learned', 'random']:\n",
    "    if direction_type == 'random':\n",
    "        samples = harm_8[(harm_8['direction_type'] == direction_type) & (harm_8['random_trial'] == 0.0)].head(2)\n",
    "    else:\n",
    "        samples = harm_8[harm_8['direction_type'] == direction_type].head(2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{direction_type.upper()}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    for idx, row in samples.iterrows():\n",
    "        print(f\"\\nPrompt: {row['prompt'][:80]}...\")\n",
    "        print(f\"Completion: {row['completion'][:200]}...\")\n",
    "        print(f\"Is Refusal: {row['is_refusal']} (score: {row['refusal_score']:.2f})\")\n",
    "        print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download Results\n",
    "\n",
    "Download all baseline pack results as a ZIP file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ZIP of all baseline results\n",
    "!zip -r baseline_pack_medium_results.zip artifacts/baselines_medium/\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('baseline_pack_medium_results.zip')\n",
    "\n",
    "print(\"\\n✓ Results downloaded!\")\n",
    "print(\"\\nExtract this ZIP and use the following files for your MATS application:\")\n",
    "print(\"  - artifacts/baselines_medium/figures/direction_specificity.png (main figure)\")\n",
    "print(\"  - artifacts/baselines_medium/figures/baseline_pack_table.csv (summary table)\")\n",
    "print(\"  - artifacts/baselines_medium/baseline_pack_results.parquet (full data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Comparison (if you ran GPT-2 Small)\n",
    "\n",
    "Compare results across model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if GPT-2 Small results exist\n",
    "if os.path.exists('artifacts/baselines/baseline_pack_results.parquet'):\n",
    "    df_small = pd.read_parquet('artifacts/baselines/baseline_pack_results.parquet')\n",
    "    df_medium = pd.read_parquet('artifacts/baselines_medium/baseline_pack_results.parquet')\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"MODEL COMPARISON: GPT-2 Small vs Medium (α=8, harm_test)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model_name, df in [('Small', df_small), ('Medium', df_medium)]:\n",
    "        harm_8 = df[(df['alpha'] == 8) & (df['split'] == 'harm_test')]\n",
    "        \n",
    "        learned = harm_8[harm_8['direction_type'] == 'learned']['is_refusal'].mean()\n",
    "        random = harm_8[harm_8['direction_type'] == 'random']['is_refusal'].mean()\n",
    "        gap = learned - random\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Learned: {learned:.1%}\")\n",
    "        print(f\"  Random:  {random:.1%}\")\n",
    "        print(f\"  Gap:     {gap:+.1%}\")\nelse:\n",
    "    print(\"GPT-2 Small results not found. Run the small model notebook first for comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
