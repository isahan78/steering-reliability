{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Pack: Llama-2 7B\n",
    "\n",
    "This notebook runs baseline experiments on **Llama-2 7B** to test direction specificity.\n",
    "\n",
    "**Model:** Llama-2 7B (7B parameters, 32 layers)\n",
    "\n",
    "**⚠️ IMPORTANT:** Llama-2 requires HuggingFace authentication\n",
    "1. Accept license: https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "2. Get HF token: https://huggingface.co/settings/tokens\n",
    "3. Enter token in cell below\n",
    "\n",
    "**Expected runtime:** ~60-90 minutes on T4 GPU, ~35-50 minutes on A100\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. HuggingFace Authentication (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LLAMA-2 AUTHENTICATION REQUIRED\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. Accept license: https://huggingface.co/meta-llama/Llama-2-7b-hf\")\n",
    "print(\"2. Get token: https://huggingface.co/settings/tokens\")\n",
    "print(\"3. Enter token below:\\n\")\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/isahan78/steering-reliability.git\n",
    "%cd steering-reliability\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall conflicting packages\n",
    "!pip uninstall -y numpy pandas datasets transformer-lens transformers pyarrow scikit-learn -q\n",
    "\n",
    "# Install dependencies\n",
    "!pip install --no-cache-dir numpy pandas torch transformer-lens transformers datasets matplotlib seaborn pyyaml tqdm pyarrow scikit-learn accelerate\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "sys.path.insert(0, '/content/steering-reliability/src')\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Memory: {mem_gb:.1f} GB\")\n",
    "    \n",
    "    if mem_gb < 14:\n",
    "        print(\"\\n⚠️  WARNING: Llama-2 7B requires ~15GB VRAM.\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Sufficient memory for Llama-2 7B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Verify Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/steering-reliability/src')\n",
    "\n",
    "from steering_reliability.config import load_config\n",
    "from steering_reliability.model import load_model\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(\"\\nReady to run Llama-2 7B baseline pack!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run Baseline Pack - Llama-2 7B\n",
    "\n",
    "This will:\n",
    "1. Load **Llama-2 7B** (~13GB download)\n",
    "2. Build learned direction\n",
    "3. Build shuffled/random/benign baselines\n",
    "4. Test all directions with ablation\n",
    "\n",
    "**Settings:**\n",
    "- Model: meta-llama/Llama-2-7b-hf\n",
    "- Layer: 16 (middle layer of 32)\n",
    "- Alphas: {0, 1, 4, 8}\n",
    "- Prompts: 50 harm_test, 50 benign\n",
    "- Random trials: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = '/content/steering-reliability/src'\n",
    "\n",
    "!PYTHONPATH=/content/steering-reliability/src python scripts/run_baseline_pack.py \\\n",
    "  --config configs/llama2_7b_baseline_pack.yaml \\\n",
    "  --layer 16 \\\n",
    "  --alphas 0 1 4 8 \\\n",
    "  --n_harm_test 50 \\\n",
    "  --n_benign 50 \\\n",
    "  --n_random 10 \\\n",
    "  --seed 0 \\\n",
    "  --include_benign_contrast \\\n",
    "  --output_dir artifacts/baselines_llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m steering_reliability.analysis.plot_baseline_pack \\\n",
    "  --in_parquet artifacts/baselines_llama2/baseline_pack_results.parquet \\\n",
    "  --out_dir artifacts/baselines_llama2/figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load results\n",
    "df = pd.read_parquet('artifacts/baselines_llama2/baseline_pack_results.parquet')\n",
    "table = pd.read_csv('artifacts/baselines_llama2/figures/baseline_pack_table.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LLAMA-2 7B BASELINE PACK RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(table.to_string(index=False))\n",
    "\n",
    "# Key findings\n",
    "harm_8 = df[(df['alpha'] == 8) & (df['split'] == 'harm_test')]\n",
    "learned = harm_8[harm_8['direction_type'] == 'learned']['is_refusal'].mean()\n",
    "random = harm_8[harm_8['direction_type'] == 'random']['is_refusal'].mean()\n",
    "gap = learned - random\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"KEY RESULTS (α=8, harm_test)\")\n",
    "print('='*80)\n",
    "print(f\"Learned: {learned:.1%}\")\n",
    "print(f\"Random:  {random:.1%}\")\n",
    "print(f\"Gap:     {gap:+.1%}\")\n",
    "\n",
    "if gap > 0.3:\n",
    "    print(\"\\n✅ STRONG: Direction is highly specific!\")\n",
    "elif gap > 0.1:\n",
    "    print(\"\\n⚠️  MODERATE: Direction shows some specificity\")\n",
    "else:\n",
    "    print(\"\\n❌ WEAK: Random matches learned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SAMPLE COMPLETIONS (α=8, harm_test)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for direction_type in ['learned', 'random']:\n",
    "    if direction_type == 'random':\n",
    "        samples = harm_8[(harm_8['direction_type'] == direction_type) & (harm_8['random_trial'] == 0.0)].head(3)\n",
    "    else:\n",
    "        samples = harm_8[harm_8['direction_type'] == direction_type].head(3)\n",
    "    \n",
    "    print(f\"\\n{direction_type.upper()}:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for idx, row in samples.iterrows():\n",
    "        print(f\"\\nPrompt: {row['prompt'][:70]}...\")\n",
    "        print(f\"Completion: {row['completion'][:150]}...\")\n",
    "        print(f\"Refusal: {row['is_refusal']} (score: {row['refusal_score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"DIRECTION SPECIFICITY\")\n",
    "display(Image(filename='artifacts/baselines_llama2/figures/direction_specificity.png'))\n",
    "\n",
    "print(\"\\nBENIGN PRESERVATION\")\n",
    "display(Image(filename='artifacts/baselines_llama2/figures/benign_preservation.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r llama2_baseline_results.zip artifacts/baselines_llama2/\n",
    "\n",
    "from google.colab import files\n",
    "files.download('llama2_baseline_results.zip')\n",
    "\n",
    "print(\"✅ Results downloaded!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
